{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCZYXwtCsL_y"
      },
      "source": [
        "CA02: This is a eMail Spam Classifers that uses Naive Bayes supervised machine learning algorithm.\n",
        "\n",
        "In this assignment you will ...\n",
        "1. Complete the code such a way that it works correctly with this given parts of the program.\n",
        "2. Explain as clearly as possible what each part of the code is doing. Use \"Markdown\" texts and code commenting to explain the code\n",
        "\n",
        "IMPORTANT NOTE:\n",
        "\n",
        "The path of your data folders 'train-mails' and 'test-mails' must be './train-mails' and './test-mails'. This means you must have your .ipynb file and these folders in the SAME FOLDER in your laptop or Google Drive. The reason for doing this is, this way the peer reviewes and I would be able to run your code from our computers using this exact same relative path, irrespective of our folder hierarchy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4p_DvtT7sOIr",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "4e14459e-d06c-4db3-ba54-6b80495cc2b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# Import all other necessary libraries. Your code below ...\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "id": "jjKF0nIMwz8_",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "def make_Dictionary(root_dir):\n",
        "  all_words = []#empty dic for all our words found in the emails\n",
        "  emails = [os.path.join(root_dir,f) for f in os.listdir(root_dir)] #looks in root dir to find all the email files\n",
        "  for mail in emails:\n",
        "    with open(mail) as m:\n",
        "      for line in m:\n",
        "        words = line.split()#for each email oppen and read it by line\n",
        "        all_words += words #collecting all words and adding it to all words\n",
        "  dictionary = Counter(all_words)#counts how many times each word shows up\n",
        "  list_to_remove = list(dictionary)#creates a list of words in the dictionary\n",
        "\n",
        "  for item in list_to_remove: #loop checks each word in the list to remove\n",
        "    if item.isalpha() == False:#if it has non letter characters like numbers or symbols delete it\n",
        "      del dictionary[item]\n",
        "    elif len(item) == 1:#if the word is one letter long also delete it\n",
        "      del dictionary[item]\n",
        "  dictionary = dictionary.most_common(3000)#most common is a method in counter that retrieves the most popopular 3000 words\n",
        "  return dictionary\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "id": "dmVW5xNlyOFc",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "def extract_features(mail_dir):#mail dir will be a folder full of emails\n",
        "  files = [os.path.join(mail_dir,fi) for fi in os.listdir(mail_dir)]#look inside these folders and make a list of paths to evbery email file\n",
        "  features_matrix = np.zeros((len(files),3000))#rows are number of emails and columns are gonna be the 3000 popular words we care about\n",
        "  train_labels = np.zeros(len(files))#another list of zeroes, one number per email, 0 not spam 1 spam\n",
        "  count = 1;#unused helper according to AI, not needed\n",
        "  docID = 0;#which email am i working on right now?\n",
        "  for fil in files:#or each email file\n",
        "    with open(fil) as fi:#open the email so we can read it\n",
        "      for i, line in enumerate(fi):#i is the line number, line is the actual text, so read the email line by line\n",
        "        if i ==2:#so start looking at line 3 skip the first two, this is cuz counting starts at 0 and we are going straight to the body of the email\n",
        "          words = line.split()#split the sentence into seperate words\n",
        "          for word in words:#look at eaech word\n",
        "            wordID = 0#where which column the word belongs to\n",
        "            for i, d in enumerate(dictionary): #d is the actual word here, go through the dictionary of most popular 3k words\n",
        "              if d[0] == word: #if the word matches one of our 3000\n",
        "                wordID = i #remember which column it belongs to\n",
        "                features_matrix[docID,wordID] = words.count(word) #count how many times it shows up in this email\n",
        "      train_labels[docID] = 0; #assume it is not spam\n",
        "      filepathTokens = fil.split('/') #break the file path into pieces, i dont really get why\n",
        "      lastToken = filepathTokens[len(filepathTokens)-1] #get the file name\n",
        "      if lastToken.startswith(\"spmsg\"): #if the file name starts with spmsg\n",
        "        train_labels[docID] = 1;#mark as spam\n",
        "        count = count + 1#move onto the next and count it\n",
        "      docID = docID + 1\n",
        "  return features_matrix, train_labels     #return the big table of word couns, and the list of spam or not spam labels\n",
        "  #email number will be the rows and the count of words will be the columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "collapsed": true,
        "id": "zoq-rE7Mx0pp",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# Enter the \"path\" of your \"train_mails\" and \"test-mails\" FOLDERS in this cell ...\n",
        "# for example: TRAIN_DIR = '../../train-mails'\n",
        "#              TEST_DIR = '../../test-mails'\n",
        "TRAIN_DIR = \"/content/drive/MyDrive/train-mails\"\n",
        "TEST_DIR = \"/content/drive/MyDrive/test-mails\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "134lmhauyQxE",
        "outputId": "edd167cb-9544-4e61-b2f1-1b83faac9a60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reading and processing emails from TRAIN and TEST folders\n"
          ]
        }
      ],
      "source": [
        "dictionary = make_Dictionary(TRAIN_DIR)\n",
        "\n",
        "print (\"reading and processing emails from TRAIN and TEST folders\")\n",
        "features_matrix, labels = extract_features(TRAIN_DIR)#big table of word counts\n",
        "test_features_matrix, test_labels = extract_features(TEST_DIR)#labels spam or not spam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "83cce6a6-aff5-4e93-ef0a-700606437aa9",
        "id": "z-pwtM6HD3Ry"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "reading and processing emails from TRAIN and TEST folders\n",
            "Training Model using Gaussian Naibe Bayes algorithm .....\n",
            "Training completed\n",
            "testing trained model to predict Test Data labels\n",
            "Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\n",
            "0.9653846153846154\n"
          ]
        }
      ],
      "source": [
        "# In this section enter your code to TRAIN the model using Naive Bayes algorithm, then PREDICT and then evaluate PERFORMANCE (Accuracy)\n",
        "# Your code below ...\n",
        "#I will keep this as text to explain what will be done next, as per the output we will be using the Gaussian Bayes  algorithm\n",
        "#What it does is look at the compilation of words and asks the simple question on whether the email is likely spam or not\n",
        "#It learns from our 3000 dictionary how many times these words show up in spam emails, and in contrary how many they show up in normal ones\n",
        "# Your output should look like below if your code is right"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(\"Training Model using Gaussian Naive Bayes algorithm .....\")\n",
        "\n",
        "# 1) create the model\n",
        "model = GaussianNB()\n",
        "\n",
        "# 2) train (learn patterns from the training data)\n",
        "model.fit(features_matrix, labels)\n",
        "\n",
        "print(\"Training completed\")\n",
        "print(\"testing trained model to predict Test Data labels\")\n",
        "\n",
        "# 3) predict labels for the test data\n",
        "predicted_labels = model.predict(test_features_matrix)\n",
        "\n",
        "print(\"Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\")\n",
        "\n",
        "# 4) evaluate performance (accuracy)\n",
        "acc = accuracy_score(test_labels, predicted_labels)\n",
        "print(acc)\n",
        "\n",
        "print(\"================ END OF PROGRAM =================\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywjnlr_TEFFJ",
        "outputId": "bbabf4fb-f78f-4b34-d699-cbb3370bde16"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Model using Gaussian Naive Bayes algorithm .....\n",
            "Training completed\n",
            "testing trained model to predict Test Data labels\n",
            "Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\n",
            "0.9653846153846154\n",
            "================ END OF PROGRAM =================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5_mPrvN586A"
      },
      "source": [
        "======================= END OF PROGRAM ========================="
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The start of my revised code. AI assisted to find better optimization and other solutions I was not yet privy to."
      ],
      "metadata": {
        "id": "Ah5OSA5xJMaD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# Import all other necessary libraries. Your code below ...\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n"
      ],
      "metadata": {
        "id": "AJiPFpNQJMyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import os\n",
        "\n",
        "def make_Dictionary(root_dir):\n",
        "    all_words = []\n",
        "    emails = [os.path.join(root_dir, f) for f in os.listdir(root_dir)]\n",
        "\n",
        "    for mail in emails:\n",
        "        #this is the first major change, this was done to prevent crashes when emails have weird characters to ignore them\n",
        "        with open(mail, \"r\", errors=\"ignore\") as m:\n",
        "            for line in m:\n",
        "                words = line.split()\n",
        "                all_words += words\n",
        "\n",
        "    dictionary = Counter(all_words)\n",
        "\n",
        "    #major change 2, in the past code we had the list_to_remove and then the for loop under it\n",
        "    #the old code deleted the words as we went through them which is seen as unsafe in coding\n",
        "    #the way around that was list_to_remove= list(dictionary), which was like setting them aside and then deleting them\n",
        "    #we are maintaining the low count words that pose no risk and making them the safe word list, no need for deletion and risk of loss of data\n",
        "    dictionary = Counter({\n",
        "        word: count\n",
        "        for word, count in dictionary.items()\n",
        "        if word.isalpha() and len(word) > 1\n",
        "    })\n",
        "\n",
        "    dictionary = dictionary.most_common(3000)\n",
        "    return dictionary\n"
      ],
      "metadata": {
        "id": "UollX0CPJSFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this optimization was done in assistance with AI to find faster solutions and help reduce the redundant loops in the code\n",
        "def extract_features(mail_dir, dictionary, n_words=3000):\n",
        "    \"\"\"\n",
        "    Turns each email file into a row of word-count features.\n",
        "    - mail_dir: folder containing email files\n",
        "    - dictionary: list like [(word, count), ...] from make_Dictionary\n",
        "    - n_words: number of dictionary words (default 3000)\n",
        "    \"\"\"\n",
        "    files = [os.path.join(mail_dir, f) for f in os.listdir(mail_dir)]\n",
        "    features_matrix = np.zeros((len(files), n_words), dtype=float)\n",
        "    labels = np.zeros(len(files), dtype=int)\n",
        "\n",
        "    #this helps reduce the next few lines of code take minutes (from the original)\n",
        "    #it does so by seeing a word and finding its column\n",
        "    word_to_idx = {w: i for i, (w, _) in enumerate(dictionary[:n_words])}\n",
        "\n",
        "    for docID, filepath in enumerate(files):\n",
        "        # label from filename\n",
        "        filename = os.path.basename(filepath)\n",
        "        labels[docID] = 1 if filename.startswith(\"spmsg\") else 0\n",
        "\n",
        "        # read whole email text (more robust than i==2)\n",
        "        with open(filepath, \"r\", errors=\"ignore\") as f:\n",
        "            text = f.read()\n",
        "\n",
        "        words = text.split()\n",
        "        counts = Counter(words)\n",
        "\n",
        "        # fill row using counts\n",
        "        for w, c in counts.items():\n",
        "            idx = word_to_idx.get(w) #this is what I referred to earlier in finding the columns\n",
        "            if idx is not None:\n",
        "                features_matrix[docID, idx] = c\n",
        "\n",
        "    return features_matrix, labels\n",
        "#this is faster than the original because we would check for every word, go through the entire dictionary to find where it belongs and then\n",
        "#go through the email again to count it, inneficient and pointless"
      ],
      "metadata": {
        "id": "qKoXBipzJZcJ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_DIR = \"/content/drive/MyDrive/train-mails\"\n",
        "TEST_DIR = \"/content/drive/MyDrive/test-mails\""
      ],
      "metadata": {
        "id": "xLvena88JxOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary = make_Dictionary(TRAIN_DIR)\n",
        "\n",
        "features_matrix, labels = extract_features(TRAIN_DIR, dictionary)\n",
        "test_features_matrix, test_labels = extract_features(TEST_DIR, dictionary)\n"
      ],
      "metadata": {
        "id": "Zl6K6sBeJxul"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(\"Training Model using Gaussian Naive Bayes algorithm .....\")\n",
        "\n",
        "# 1) create the model\n",
        "model = GaussianNB()\n",
        "\n",
        "# 2) train (learn patterns from the training data)\n",
        "model.fit(features_matrix, labels)\n",
        "\n",
        "print(\"Training completed\")\n",
        "print(\"testing trained model to predict Test Data labels\")\n",
        "\n",
        "# 3) predict labels for the test data\n",
        "predicted_labels = model.predict(test_features_matrix)\n",
        "\n",
        "print(\"Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\")\n",
        "\n",
        "# 4) evaluate performance (accuracy)\n",
        "acc = accuracy_score(test_labels, predicted_labels)\n",
        "print(acc)\n",
        "\n",
        "print(\"================ END OF PROGRAM =================\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCcSTGqEKHWa",
        "outputId": "9c2eb1fd-2a0d-4b08-bc05-5661f8a8979b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Model using Gaussian Naive Bayes algorithm .....\n",
            "Training completed\n",
            "testing trained model to predict Test Data labels\n",
            "Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\n",
            "0.9653846153846154\n",
            "================ END OF PROGRAM =================\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
